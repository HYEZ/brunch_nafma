{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import glob\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('./res/writer_user_doc.txt')\n",
    "\n",
    "words = []\n",
    "for f in files:\n",
    "    file = open(f)\n",
    "    words.append(file.read())\n",
    "    file.close()\n",
    "\n",
    "words = list(chain.from_iterable(words))\n",
    "words = ''.join(words)[:-1]\n",
    "sentences = words.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df['user'] = sentences_df[0].apply(lambda x : x.split()[0])\n",
    "sentences_df['words'] = sentences_df[0].apply(lambda x : ' '.join(x.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>user</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@nicemerry 학교 휴직 영화 프라이팬 일상 끄적 일상 요리 환갑 일상 쉼 목...</td>\n",
       "      <td>@nicemerry</td>\n",
       "      <td>학교 휴직 영화 프라이팬 일상 끄적 일상 요리 환갑 일상 쉼 목표 충전 일상 여행 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@dltjtks 응급실 죽음 간호사</td>\n",
       "      <td>@dltjtks</td>\n",
       "      <td>응급실 죽음 간호사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@diversityinlife 생명 다양 성 재단 그림일기 윈 저성 활동가 활동 생...</td>\n",
       "      <td>@diversityinlife</td>\n",
       "      <td>생명 다양 성 재단 그림일기 윈 저성 활동가 활동 생명 도 토리 생명 다양 성 재단...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@hyokyoungko 기다림 기 도 믿 음 엄마 어버이날 생일 concert 생일...</td>\n",
       "      <td>@hyokyoungko</td>\n",
       "      <td>기다림 기 도 믿 음 엄마 어버이날 생일 concert 생일 파티 기부 친구 사람 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@erish2150 프로그래밍 개발 생각 생각 프로젝트 인간</td>\n",
       "      <td>@erish2150</td>\n",
       "      <td>프로그래밍 개발 생각 생각 프로젝트 인간</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0              user  \\\n",
       "0  @nicemerry 학교 휴직 영화 프라이팬 일상 끄적 일상 요리 환갑 일상 쉼 목...        @nicemerry   \n",
       "1                                @dltjtks 응급실 죽음 간호사          @dltjtks   \n",
       "2  @diversityinlife 생명 다양 성 재단 그림일기 윈 저성 활동가 활동 생...  @diversityinlife   \n",
       "3  @hyokyoungko 기다림 기 도 믿 음 엄마 어버이날 생일 concert 생일...      @hyokyoungko   \n",
       "4                  @erish2150 프로그래밍 개발 생각 생각 프로젝트 인간        @erish2150   \n",
       "\n",
       "                                               words  \n",
       "0  학교 휴직 영화 프라이팬 일상 끄적 일상 요리 환갑 일상 쉼 목표 충전 일상 여행 ...  \n",
       "1                                         응급실 죽음 간호사  \n",
       "2  생명 다양 성 재단 그림일기 윈 저성 활동가 활동 생명 도 토리 생명 다양 성 재단...  \n",
       "3  기다림 기 도 믿 음 엄마 어버이날 생일 concert 생일 파티 기부 친구 사람 ...  \n",
       "4                             프로그래밍 개발 생각 생각 프로젝트 인간  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df_indexed = sentences_df.reset_index().set_index('user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 19141], ('여행', 133392), ('에', 86381), ('세이', 76152), ('사랑', 60928)]\n",
      "Sample data [[122, 409, 5, 10034, 13, 6639, 13, 75, 7447, 13, 3185, 270, 1930, 13, 1, 13, 295, 1], [982, 208, 538]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 40000\n",
    "\n",
    "def build_dataset(sentences):\n",
    "    words = ''.join(sentences).split()\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    unk_count = 0\n",
    "    sent_data = []\n",
    "    for sentence in sentences:\n",
    "        data = []\n",
    "        for word in sentence.split():\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            data.append(index)\n",
    "        sent_data.append(data)\n",
    "    \n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return sent_data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(sentences_df_indexed['words'].tolist())\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:2])\n",
    "# del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5539467\n"
     ]
    }
   ],
   "source": [
    "skip_window = 5\n",
    "instances = 0\n",
    "\n",
    "# Pad sentence with skip_windows\n",
    "for i in range(len(data)):\n",
    "    data[i] = [vocabulary_size]*skip_window+data[i]+[vocabulary_size]*skip_window\n",
    "\n",
    "# Check how many training samples that we get    \n",
    "for sentence  in data:\n",
    "    instances += len(sentence)-2*skip_window\n",
    "print(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = np.zeros((instances,skip_window*2+1),dtype=np.int32)\n",
    "labels = np.zeros((instances,1),dtype=np.int32)\n",
    "doc = np.zeros((instances,1),dtype=np.int32)\n",
    "\n",
    "k = 0\n",
    "for doc_id, sentence  in enumerate(data):\n",
    "    for i in range(skip_window, len(sentence)-skip_window):\n",
    "        context[k] = sentence[i-skip_window:i+skip_window+1] # Get surrounding words\n",
    "        labels[k] = sentence[i] # Get target variable\n",
    "        doc[k] = doc_id\n",
    "        k += 1\n",
    "        \n",
    "context = np.delete(context,skip_window,1) # delete the middle word        \n",
    "        \n",
    "shuffle_idx = np.random.permutation(k)\n",
    "labels = labels[shuffle_idx]\n",
    "doc = doc[shuffle_idx]\n",
    "context = context[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "context_window = 2*skip_window\n",
    "embedding_size = 50 # Dimension of the embedding vector.\n",
    "softmax_width = embedding_size # +embedding_size2+embedding_size3\n",
    "num_sampled = 5 # Number of negative examples to sample.\n",
    "sum_ids = np.repeat(np.arange(batch_size),context_window)\n",
    "\n",
    "len_docs = len(data)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(): # , tf.device('/cpu:0')\n",
    "    # Input data.\n",
    "    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size*context_window])\n",
    "    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    segment_ids = tf.constant(sum_ids, dtype=tf.int32)\n",
    "\n",
    "    word_embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "    word_embeddings = tf.concat([word_embeddings,tf.zeros((1,embedding_size))],0)\n",
    "    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))\n",
    "\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, softmax_width],\n",
    "                             stddev=1.0 / np.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed_words = tf.segment_mean(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),segment_ids)\n",
    "    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)\n",
    "    embed = (embed_words+embed_docs)/2.0#+embed_hash+embed_users\n",
    "\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(softmax_weights, softmax_biases, train_labels, \n",
    "                                         embed, num_sampled, vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "        \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(doc_embeddings), 1, keep_dims=True))\n",
    "    normalized_doc_embeddings = doc_embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################\n",
    "# Chunk the data to be passed into the tensorflow Model\n",
    "###########################\n",
    "data_idx = 0\n",
    "def generate_batch(batch_size):\n",
    "    global data_idx\n",
    "\n",
    "    if data_idx+batch_size<instances:\n",
    "        batch_labels = labels[data_idx:data_idx+batch_size]\n",
    "        batch_doc_data = doc[data_idx:data_idx+batch_size]\n",
    "        batch_word_data = context[data_idx:data_idx+batch_size]\n",
    "        data_idx += batch_size\n",
    "    else:\n",
    "        overlay = batch_size - (instances-data_idx)\n",
    "        batch_labels = np.vstack([labels[data_idx:instances],labels[:overlay]])\n",
    "        batch_doc_data = np.vstack([doc[data_idx:instances],doc[:overlay]])\n",
    "        batch_word_data = np.vstack([context[data_idx:instances],context[:overlay]])\n",
    "        data_idx = overlay\n",
    "    batch_word_data = np.reshape(batch_word_data,(-1,1))\n",
    "\n",
    "    return batch_labels, batch_word_data, batch_doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 39.481388\n",
      "Average loss at step 10000: 16.504855\n",
      "Average loss at step 20000: 10.137205\n",
      "Average loss at step 30000: 7.782632\n",
      "Average loss at step 40000: 6.338275\n",
      "Average loss at step 50000: 5.252970\n",
      "Average loss at step 60000: 4.594733\n",
      "Average loss at step 70000: 4.111236\n",
      "Average loss at step 80000: 3.755370\n",
      "Average loss at step 90000: 3.412666\n",
      "Average loss at step 100000: 3.143643\n",
      "Average loss at step 110000: 2.901804\n",
      "Average loss at step 120000: 2.884862\n",
      "Average loss at step 130000: 2.625388\n",
      "Average loss at step 140000: 2.487165\n",
      "Average loss at step 150000: 2.382428\n",
      "Average loss at step 160000: 2.302203\n",
      "Average loss at step 170000: 2.210721\n",
      "Average loss at step 180000: 2.177297\n",
      "Average loss at step 190000: 2.111760\n",
      "Average loss at step 200000: 2.025514\n"
     ]
    }
   ],
   "source": [
    "num_steps = 200001\n",
    "step_delta = int(num_steps/20)\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_labels, batch_word_data, batch_doc_data\\\n",
    "        = generate_batch(batch_size)\n",
    "        feed_dict = {train_word_dataset : np.squeeze(batch_word_data),\n",
    "                     train_doc_dataset : np.squeeze(batch_doc_data),\n",
    "                     train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % step_delta == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / step_delta\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "    save_path = tf.train.Saver().save(session, \"./doc2vec_model\")    \n",
    "\n",
    "    # Get the weights to save for later\n",
    "    final_word_embeddings = word_embeddings.eval()\n",
    "    final_word_embeddings_out = softmax_weights.eval()\n",
    "    final_doc_embeddings = normalized_doc_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999\n",
      "0.6036673\n",
      "0.603291\n",
      "0.59949213\n",
      "0.5988153\n",
      "0.59659284\n",
      "0.5926226\n",
      "0.59009594\n",
      "0.5883219\n",
      "0.58178824\n"
     ]
    }
   ],
   "source": [
    "rand_doc = np.random.randint(len_docs)\n",
    "dist = final_doc_embeddings.dot(final_doc_embeddings[rand_doc][:,None])\n",
    "closest_doc = np.argsort(dist,axis=0)[-10:][::-1]\n",
    "furthest_doc = np.argsort(dist,axis=0)[0][::-1]\n",
    "\n",
    "for idx in closest_doc:\n",
    "    print(dist[idx][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(user_id, size):\n",
    "    user_index = sentences_df_indexed.loc[user_id]['index']\n",
    "    dist = final_doc_embeddings.dot(final_doc_embeddings[user_index][:,None])\n",
    "    closest_doc = np.argsort(dist,axis=0)[-size:][::-1]\n",
    "    furthest_doc = np.argsort(dist,axis=0)[0][::-1]\n",
    "\n",
    "    result = []\n",
    "    for idx, item in enumerate(closest_doc):\n",
    "        user = sentences[closest_doc[idx][0]].split()[0]\n",
    "        dist_value = dist[item][0][0]\n",
    "        result.append([user, dist_value])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['@sangjunleeinfo', 1.0000001],\n",
       " ['@mylife4iu', 0.5428308],\n",
       " ['@johnbird', 0.51903224],\n",
       " ['@ongrim', 0.51678944],\n",
       " ['@hash-on', 0.5149888],\n",
       " ['@kimikim', 0.5143247],\n",
       " ['@yerinirenekang', 0.5140254],\n",
       " ['@hillitoot', 0.5125634],\n",
       " ['@wkwn71', 0.5020619],\n",
       " ['@bookspause', 0.50160414]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_doc(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21422"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df_indexed.loc['#d6866a498157771069fdf15361cb012b']['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#d6866a498157771069fdf15361cb012b', 1.0000001],\n",
       " ['#ac8fc562dac1a5c6ef86dfc616db6b45', 0.6216059],\n",
       " ['@hwiki', 0.6086082],\n",
       " ['@aidfox', 0.6038921],\n",
       " ['#29a089a9401ad7e26b20d5df89f10332', 0.60082906],\n",
       " ['@thsgmlgns2', 0.59521574],\n",
       " ['@kierstenlee', 0.5884473],\n",
       " ['@wkdanswjd102', 0.57278204],\n",
       " ['@seongminyoo', 0.5682178],\n",
       " ['#2afdcfd40f7d105c79ff68b7c3c52f65', 0.5680978]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('#d6866a498157771069fdf15361cb012b', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#87a6479c91e4276374378f1d28eb307c', 0.9999999],\n",
       " ['@parkkimsoojin', 0.570564],\n",
       " ['@ttwakid', 0.5545074],\n",
       " ['#7f4912b56ff1e9cf0bfff5067321c9d9', 0.5463936],\n",
       " ['@wavejdh', 0.5359304],\n",
       " ['@hyunikun', 0.5332689],\n",
       " ['@chogh31', 0.52276725],\n",
       " ['@jonylee', 0.5210302],\n",
       " ['@parkhyungsik', 0.5201211],\n",
       " ['@machinoh', 0.51795894]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('#87a6479c91e4276374378f1d28eb307c', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['#a0df5bd0e5a5bbc28b87f8c64462667c', 1.0000001],\n",
       " ['@hss730', 0.65026903],\n",
       " ['@joy2003bqji', 0.6284451],\n",
       " ['@inqatar', 0.61798096],\n",
       " ['@brunch9d21', 0.61397386],\n",
       " ['@miriaemoon', 0.61324745],\n",
       " ['@elara1020', 0.6099669],\n",
       " ['@ike', 0.607323],\n",
       " ['@myfriendjesus', 0.5987311],\n",
       " ['#ce81bac9c227d00bb554a2463bcb117e', 0.5972982]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('#a0df5bd0e5a5bbc28b87f8c64462667c', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(user_id, writer_id):\n",
    "    user_index = sentences_df_indexed.loc[user_id]['index']\n",
    "    writer_index = sentences_df_indexed.loc[writer_id]['index']\n",
    "    dist = final_doc_embeddings[user_index].dot(final_doc_embeddings[writer_index])\n",
    "    #print('{} - {} : {}'.format(user_id, writer_id, dist))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#d6866a498157771069fdf15361cb012b - @brunch : 0.16178107261657715\n"
     ]
    }
   ],
   "source": [
    "similar('#d6866a498157771069fdf15361cb012b', '@brunch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#d6866a498157771069fdf15361cb012b - @seochogirl : 0.28863343596458435\n"
     ]
    }
   ],
   "source": [
    "similar('#d6866a498157771069fdf15361cb012b', '@seochogirl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('doc_embeddings', final_doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
